# -*- coding: utf-8 -*-
"""Neural Signal Decoding.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1q8wl3EjkgTmTOLojcfE-fitpCF_RFQgh

***CNN Classfiier for Motor Imagery***
"""

pip install mne torch torchvision imbalanced-learn

import numpy as np
import matplotlib.pyplot as plt
import mne
from mne.datasets import eegbci
from mne.io import read_raw_edf
import os
import torch
import torch.nn as nn
import torch.optim as optim
from torch.optim import lr_scheduler
from torch.optim.lr_scheduler import ReduceLROnPlateau
from torch.utils.data import DataLoader, TensorDataset
from sklearn.preprocessing import StandardScaler, RobustScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, accuracy_score, confusion_matrix, ConfusionMatrixDisplay
from sklearn.utils.class_weight import compute_class_weight
import seaborn as sns
import pandas as pd
from mne.decoding import Scaler
from imblearn.over_sampling import SMOTE

from google.colab import drive
drive.mount('/content/drive')

import glob
import os

drive_data_path = '/content/drive/MyDrive/physionet_mi'

edf_files = glob.glob(os.path.join(drive_data_path, '**', '*.edf'), recursive=True)

print(f"Found {len(edf_files)} EDF files")
subject_folders = [f"S{str(i).zfill(3)}" for i in range(1, 110)]

files = []
for subject in subject_folders:
    subject_path = os.path.join(drive_data_path, subject)
    files += glob.glob(os.path.join(subject_path, '*.edf'))

print(f"Total EDF files found from 110 subjects: {len(files)}")

file = files[0]
raw = mne.io.read_raw_edf(file, preload=True, verbose=False)
event_id = {'T1': 1, 'T2': 2}
events, _ = mne.events_from_annotations(raw, event_id=event_id)
epochs = mne.Epochs(raw, events, event_id=event_id, tmin=1.0, tmax=4.0,
                    baseline=None, preload=True, verbose=False)

fig, axes = plt.subplots(2, 1, figsize=(12, 8), sharex=True)

# Plot T1 data (manual plotting)
t1_data = epochs['T1'].get_data()[0]  # Get first epoch data [n_channels, n_times]
t2_data = epochs['T2'].get_data()[0]
times = epochs.times

# Plot first 10 channels for T1
for ch_idx in range(10):
    axes[0].plot(times, t1_data[ch_idx] + ch_idx * 1e-4, label=f'Ch {ch_idx+1}')
axes[0].set_title('Left Fist (T1) - Raw')
axes[0].legend(loc='upper right', bbox_to_anchor=(1.15, 1))
axes[0].set_ylabel('Amplitude (uV)')

# Plot first 10 channels for T2
for ch_idx in range(10):
    axes[1].plot(times, t2_data[ch_idx] + ch_idx * 1e-4, label=f'Ch {ch_idx+1}')
axes[1].set_title('Right Fist (T2) - Raw')
axes[1].set_xlabel('Time (s)')
axes[1].set_ylabel('Amplitude (uV)')

plt.tight_layout()
plt.show()

def preprocess_data(files, low_cut=8.0, high_cut=30.0, tmin=1.0, tmax=4.0, sfreq=160):
    # Initialize containers
    all_epochs = []
    all_labels = []
    class_counts = {'Left': 0, 'Right': 0}
    dropped_files = 0

    for file in files:
        try:
            # 1. Load and basic preprocessing
            raw = mne.io.read_raw_edf(file, preload=True, verbose=True)

            # 2. Debug channel names
            print("Available channels:", raw.ch_names)

            # 3. Flexible channel selection with dot handling
            target_channels = ['C3', 'C4', 'Cz', 'FC3', 'FC4', 'CP3', 'CP4']
            available_channels = [ch for ch in raw.ch_names
                               if any(tc in ch.replace('.', '') for tc in target_channels)]

            if len(available_channels) < 3:
                print(f"⚠️ Skipping {file}: Only found {len(available_channels)} target channels")
                dropped_files += 1
                continue

            raw.pick(available_channels)
            print("Selected channels:", raw.ch_names)

            # 4. Advanced filtering
            raw.filter(low_cut, high_cut, method='iir', iir_params=dict(order=4, ftype='butter'))

            # 5. Notch filter only at possible frequencies
            nyquist = sfreq / 2
            notch_freqs = [f for f in [50] if f < nyquist]
            if notch_freqs:
                raw.notch_filter(notch_freqs)

            # 6. Resampling
            raw.resample(sfreq, npad='auto')

            # 7. Event handling with flexible mapping
            events, event_mapping = mne.events_from_annotations(raw)
            event_id = {
                'left': event_mapping.get('T1', event_mapping.get('left', -1)),
                'right': event_mapping.get('T2', event_mapping.get('right', -1))
            }
            event_id = {k:v for k,v in event_id.items() if v != -1}

            if not event_id:
                dropped_files += 1
                continue

            # 8. Epoching with corrected baseline
            # Key fix: baseline period must be (start, end) where start < end
            # Since your tmin=1.0 and event is at 0, use (-1.0, 0.0) for 1s pre-stimulus baseline
            epochs = mne.Epochs(
                raw,
                events,
                event_id=event_id,
                tmin=-1.0,  # Start 1s before event
                tmax=3.0,   # End 3s after event (total 4s window)
                baseline=(-1.0, 0.0),  # Baseline is pre-stimulus period
                reject_by_annotation=True,
                preload=True,
                verbose=False
            )

            if len(epochs) < 5:  # Minimum epochs threshold
                dropped_files += 1
                continue

            # 9. Advanced scaling per channel
            scaler = Scaler(scalings='median')  # Robust to outliers
            epochs_data = scaler.fit_transform(epochs.get_data())

            # 10. Label handling
            labels = []
            for event in epochs.events[:, 2]:
                if event == event_id.get('left'):
                    labels.append(0)
                    class_counts['Left'] += 1
                else:
                    labels.append(1)
                    class_counts['Right'] += 1

            all_epochs.append(epochs_data)
            all_labels.extend(labels)

        except Exception as e:
            print(f"❌ Error in {file}: {str(e)}")
            dropped_files += 1
            continue

    if not all_epochs:
        raise ValueError("No valid data found in any files!")

    X = np.concatenate(all_epochs)
    y = np.array(all_labels)

    print("\n=== Class Distribution BEFORE Balancing ===")
    print(f"Left: {class_counts['Left']} samples ({class_counts['Left']/len(y)*100:.1f}%)")
    print(f"Right: {class_counts['Right']} samples ({class_counts['Right']/len(y)*100:.1f}%)")

    imbalance_threshold = 0.3
    if abs(class_counts['Left'] - class_counts['Right']) / len(y) > imbalance_threshold:
        print("\n⚠️ Applying SMOTE to balance classes...")
        from imblearn.over_sampling import SMOTE
        sm = SMOTE(random_state=42)
        X_reshaped = X.reshape(X.shape[0], -1)  # Flatten for SMOTE
        X_balanced, y_balanced = sm.fit_resample(X_reshaped, y)
        X_balanced = X_balanced.reshape(-1, X.shape[1], X.shape[2])

        print("\n=== Class Distribution AFTER Balancing ===")
        unique, counts = np.unique(y_balanced, return_counts=True)
        print(f"Left: {counts[0]} samples ({counts[0]/len(y_balanced)*100:.1f}%)")
        print(f"Right: {counts[1]} samples ({counts[1]/len(y_balanced)*100:.1f}%)")

        return X_balanced, y_balanced, class_counts

    print("\n✅ Classes are already balanced within threshold")
    return X, y, class_counts

X, y, class_counts = preprocess_data(files)

# Verify output shapes
print("\n=== Final Data Shapes ===")
print(f"X shape: {X.shape}")  # Should be (n_samples, n_channels, n_times)
print(f"y shape: {y.shape}")

labels = list(class_counts.keys())
sizes = list(class_counts.values())

fig, ax = plt.subplots(figsize=(4, 4))
ax.pie(sizes, labels=labels, autopct='%1.1f%%',
        startangle=90, shadow=True, explode=(0.05, 0.05),
        colors=['#3498db', '#e74c3c'])

ax.axis('equal')

plt.title('Distribution of Motor Imagery Classes', fontsize=14)

plt.legend(loc='lower right')

total = sum(sizes)
print(f"\nClass Distribution:")
for label, size in zip(labels, sizes):
    print(f"{label}: {size} trials ({size/total*100:.1f}%)")

plt.tight_layout()
plt.show()

left_sample = X[y == 0][0]  # First left-class sample
right_sample = X[y == 1][0]  # First right-class sample
times = np.linspace(1.0, 4.0, X.shape[2])  # Time vector (1-4s)

# Create figure
plt.figure(figsize=(15, 10))

# Plot left fist (C3, C4, Cz)
plt.subplot(2, 1, 1)
plt.plot(times, left_sample[0], label='C3')  # C3 channel
plt.plot(times, left_sample[1], label='C4')  # C4 channel
plt.plot(times, left_sample[2], label='Cz')  # Cz channel
plt.title('Left Fist Imagery (ERD in C3)', fontsize=14)
plt.xlabel('Time (s)', fontsize=12)
plt.ylabel('Amplitude (μV)', fontsize=12)
plt.axvspan(2.0, 4.0, alpha=0.1, color='red')  # Highlight imagery period
plt.grid(True)
plt.legend()

# Plot right fist (C3, C4, Cz)
plt.subplot(2, 1, 2)
plt.plot(times, right_sample[0], label='C3')
plt.plot(times, right_sample[1], label='C4')
plt.plot(times, right_sample[2], label='Cz')
plt.title('Right Fist Imagery (ERD in C4)', fontsize=14)
plt.xlabel('Time (s)', fontsize=12)
plt.ylabel('Amplitude (μV)', fontsize=12)
plt.axvspan(2.0, 4.0, alpha=0.1, color='red')
plt.grid(True)
plt.legend()

plt.tight_layout()
plt.show()

# ----------------------
# Data Preparation
# ----------------------

# Verify original data shapes
print(f"Original X shape: {X.shape}")  # (19670, 3, 641)
print(f"Original y shape: {y.shape}")  # (19670,)

# Train/Val/Test Split (70/15/15)
X_temp, X_test, y_temp, y_test = train_test_split(
    X, y, test_size=0.15, random_state=42, stratify=y)

X_train, X_val, y_train, y_val = train_test_split(
    X_temp, y_temp, test_size=0.1765, random_state=42, stratify=y_temp)  # 0.1765*0.85≈0.15

print("\n=== Data Splits ===")
print(f"Train: {len(X_train)} ({len(X_train)/len(X)*100:.1f}%)")
print(f"Val: {len(X_val)} ({len(X_val)/len(X)*100:.1f}%)")
print(f"Test: {len(X_test)} ({len(X_test)/len(X)*100:.1f}%)")

# Convert to PyTorch tensors
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"\nUsing device: {device}")

# Create tensors with correct shape [batch, channel, height, width]
X_train_tensor = torch.tensor(X_train, dtype=torch.float32).unsqueeze(1)  # Add channel dim
X_val_tensor = torch.tensor(X_val, dtype=torch.float32).unsqueeze(1)
X_test_tensor = torch.tensor(X_test, dtype=torch.float32).unsqueeze(1)

y_train_tensor = torch.tensor(y_train, dtype=torch.long)
y_val_tensor = torch.tensor(y_val, dtype=torch.long)
y_test_tensor = torch.tensor(y_test, dtype=torch.long)

print("\nTensor shapes:")
print(f"X_train: {X_train_tensor.shape}, y_train: {y_train_tensor.shape}")
print(f"X_val: {X_val_tensor.shape}, y_val: {y_val_tensor.shape}")
print(f"X_test: {X_test_tensor.shape}, y_test: {y_test_tensor.shape}")

# ----------------------
# EEG Dataset and Loaders
# ----------------------

class EEGDataset(TensorDataset):
    def __init__(self, *tensors):
        super().__init__(*tensors)
        data = tensors[0]
        # Normalize each feature channel separately (data is [batch, 1, 3, 641])
        self.mean = data.mean(dim=(0, 3), keepdim=True)  # [1, 1, 3, 1]
        self.std = data.std(dim=(0, 3), keepdim=True)    # [1, 1, 3, 1]

    def __getitem__(self, index):
        x, y = super().__getitem__(index)
        # Normalize x using the precomputed mean/std (broadcasted correctly)
        x = (x - self.mean[0]) / (self.std[0] + 1e-7)  # Use [1, 3, 641]
        return x, y

# Create datasets
train_dataset = EEGDataset(X_train_tensor, y_train_tensor)
val_dataset = EEGDataset(X_val_tensor, y_val_tensor)
test_dataset = EEGDataset(X_test_tensor, y_test_tensor)

# Data loaders
batch_size = 8
train_loader = DataLoader(
    train_dataset,
    batch_size=batch_size,
    shuffle=True,
    num_workers=2,
    persistent_workers=True
)
val_loader = DataLoader(
    val_dataset,
    batch_size=batch_size*2,
    num_workers=2
)
test_loader = DataLoader(
    test_dataset,
    batch_size=batch_size*2,
    num_workers=2
)

class EEGTransformer(nn.Module):
    def __init__(self, input_dim=3, seq_len=641, n_classes=2, d_model=128, nhead=8, num_layers=3, dropout=0.2):
        super(EEGTransformer, self).__init__()
        self.seq_len = seq_len
        self.d_model = d_model

        # Downsampling: [batch, 3, 641] -> [batch, 3, 160]
        self.downsample = nn.AvgPool1d(kernel_size=4, stride=4)
        reduced_seq_len = seq_len // 4

        # Project input_dim -> d_model
        self.input_proj = nn.Linear(input_dim, d_model)

        # Positional Encoding
        position = torch.arange(reduced_seq_len).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))
        pe = torch.zeros(1, reduced_seq_len, d_model)
        pe[0, :, 0::2] = torch.sin(position * div_term)
        pe[0, :, 1::2] = torch.cos(position * div_term)
        self.register_buffer('pe', pe)

        # Transformer Encoder
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=d_model, nhead=nhead, dim_feedforward=256,
            dropout=dropout, batch_first=True, norm_first=True
        )
        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)

        # Classification Head
        self.classifier = nn.Sequential(
            nn.LayerNorm(d_model),
            nn.Linear(d_model, 64),
            nn.GELU(),
            nn.Dropout(0.3),
            nn.Linear(64, n_classes)
        )

        self._init_weights()

    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Linear):
                nn.init.xavier_uniform_(m.weight)
                nn.init.zeros_(m.bias)
            elif isinstance(m, nn.Conv1d):
                nn.init.kaiming_normal_(m.weight, nonlinearity='relu')
                if m.bias is not None:
                    nn.init.zeros_(m.bias)

    def forward(self, x):
        # Input shape can be [batch, 3, 641] or [batch, 1, 3, 641]
        if x.dim() == 4:
            x = x.squeeze(1)  # [batch, 3, 641]

        x = self.downsample(x)            # [batch, 3, 160]
        x = x.permute(0, 2, 1)            # [batch, 160, 3]
        x = self.input_proj(x)            # [batch, 160, d_model]
        x = x + self.pe[:, :x.size(1), :] # Add positional encoding
        x = self.transformer(x)           # [batch, 160, d_model]
        x = x.mean(dim=1)                 # Global average pooling
        return self.classifier(x)

class EEGCNN(nn.Module):
    def __init__(self, n_classes=2, in_channels=3, n_timepoints=641):
        super(EEGCNN, self).__init__()

        # Temporal feature extraction
        self.temp_conv = nn.Sequential(
            nn.Conv2d(1, 16, (1, 31), padding=(0, 15)),  # [b, 16, 3, 641]
            nn.BatchNorm2d(16),
            nn.ELU(),
            nn.MaxPool2d((1, 4)),  # [b, 16, 3, 160]
            nn.Dropout(0.3)
        )

        # Spatial feature extraction
        self.spatial_conv = nn.Sequential(
            nn.Conv2d(16, 32, (in_channels, 1)),  # [b, 32, 1, 160]
            nn.BatchNorm2d(32),
            nn.ELU(),
            nn.MaxPool2d((1, 5)),  # [b, 32, 1, 32]
            nn.Dropout(0.4)
        )

        # Temporal refinement
        self.temp_refine = nn.Sequential(
            nn.Conv2d(32, 64, (1, 15), padding=(0, 7)),  # [b, 64, 1, 32]
            nn.BatchNorm2d(64),
            nn.ELU(),
            nn.Conv2d(64, 64, (1, 7), padding=(0, 3)),   # [b, 64, 1, 32]
            nn.BatchNorm2d(64),
            nn.ELU(),
            nn.MaxPool2d((1, 2)),  # [b, 64, 1, 16]
            nn.Dropout(0.5)
        )

        # Classifier
        self.classifier = nn.Sequential(
            nn.Flatten(),
            nn.Linear(64*1*16, 128),
            nn.ELU(),
            nn.Dropout(0.6),
            nn.Linear(128, n_classes)
        )

        # Initialize weights
        self._init_weights()

    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.constant_(m.bias, 0)

    def forward(self, x):
        x = self.temp_conv(x)
        x = self.spatial_conv(x)
        x = self.temp_refine(x)
        return self.classifier(x)

# Initialize model
# model = EEGCNN().to(device)
import math
model = EEGTransformer().to(device)

# Verify model
dummy_input = torch.randn(32, 1, 3, 641).to(device)
print(f"\nModel test:")
print(f"Input shape: {dummy_input.shape}")
print(f"Output shape: {model(dummy_input).shape}")  # Should be [32, 2]

# ----------------------
# Training Setup
# ----------------------

# Loss function with class weighting
class_weights = torch.tensor(
    compute_class_weight('balanced', classes=np.unique(y_train), y=y_train),
    dtype=torch.float32
).to(device)
# criterion = nn.CrossEntropyLoss(weight=class_weights)

# optimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay=1e-2)
# scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=10)

criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-2)
scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=20)


def normalize_batch(batch):
    """Normalize each trial along time dimension."""
    mean = batch.mean(dim=-1, keepdim=True)
    std = batch.std(dim=-1, keepdim=True)
    return (batch - mean) / (std + 1e-6)

def add_gaussian_noise(x, std=0.001):
    return x + torch.randn_like(x) * std

# ----------------------
# Training Loop
# ----------------------

def train_model():
    best_val_acc = 0
    patience = 5
    no_improve = 0

    history = {
        "train_loss": [],
        "train_acc": [],
        "val_loss": [],
        "val_acc": []
    }

    for epoch in range(50):
        model.train()
        train_loss, correct, total = 0, 0, 0

        for xb, yb in train_loader:
            xb, yb = xb.to(device), yb.to(device)

            optimizer.zero_grad()
            outputs = model(xb)
            loss = criterion(outputs, yb)
            loss.backward()
            optimizer.step()

            train_loss += loss.item()
            _, preds = torch.max(outputs, 1)
            correct += (preds == yb).sum().item()
            total += yb.size(0)

        train_acc = correct / total
        train_loss /= len(train_loader)

        # Validation
        model.eval()
        val_loss = 0
        val_correct = 0
        val_total = 0

        with torch.no_grad():
            for xb, yb in val_loader:
                xb, yb = xb.to(device), yb.to(device)
                outputs = model(xb)
                loss = criterion(outputs, yb)

                val_loss += loss.item()
                _, preds = torch.max(outputs, 1)
                val_correct += (preds == yb).sum().item()
                val_total += yb.size(0)

        val_acc = val_correct / val_total
        val_loss /= len(val_loader)

        # Save history
        history["train_loss"].append(train_loss)
        history["train_acc"].append(train_acc)
        history["val_loss"].append(val_loss)
        history["val_acc"].append(val_acc)

        # Scheduler & Early Stopping
        scheduler.step(val_acc)
        if val_acc > best_val_acc:
            best_val_acc = val_acc
            no_improve = 0
            torch.save(model.state_dict(), 'best_model.pth')
        else:
            no_improve += 1
            if no_improve >= patience:
                print(f"Early stopping at epoch {epoch+1}")
                break

        print(f"Epoch {epoch+1}: Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, "
              f"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}")

    return history

# Run training
history = train_model()

# ----------------------
# Evaluation
# ----------------------

def evaluate_model():
    model.load_state_dict(torch.load('best_model.pth'))
    model.eval()

    # Test evaluation
    test_loss = 0
    test_correct = 0
    test_total = 0

    with torch.no_grad():
        for xb, yb in test_loader:
            xb, yb = xb.to(device), yb.to(device)
            outputs = model(xb)
            loss = criterion(outputs, yb)

            test_loss += loss.item()
            _, preds = torch.max(outputs, 1)
            test_correct += (preds == yb).sum().item()
            test_total += yb.size(0)

        test_acc = test_correct / test_total

    print(f"\nTest Results: Loss={test_loss/len(test_loader):.4f}, Acc={test_acc:.4f}")

    # Classification report
    from sklearn.metrics import classification_report

    # Get all predictions
    all_preds = []
    all_labels = []

    with torch.no_grad():
        for xb, yb in test_loader:
            xb, yb = xb.to(device), yb.to(device)
            outputs = model(xb)
            _, preds = torch.max(outputs, 1)
            all_preds.extend(preds.cpu().numpy())
            all_labels.extend(yb.cpu().numpy())

    print("\nClassification Report:")
    print(classification_report(
        all_labels,
        all_preds,
        target_names=['Left', 'Right']
    ))
    cm = confusion_matrix(all_labels, all_preds)
    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['Left', 'Right'])

    print("\nConfusion Matrix:")
    disp.plot(cmap='Blues')
    plt.show()

evaluate_model()

def plot_history(history):
    epochs = range(1, len(history["train_loss"]) + 1)

    plt.figure(figsize=(12, 5))

    # Loss
    plt.subplot(1, 2, 1)
    plt.plot(epochs, history["train_loss"], label="Train Loss")
    plt.plot(epochs, history["val_loss"], label="Val Loss")
    plt.xlabel("Epoch")
    plt.ylabel("Loss")
    plt.title("Loss Curve")
    plt.legend()

    # Accuracy
    plt.subplot(1, 2, 2)
    plt.plot(epochs, history["train_acc"], label="Train Acc")
    plt.plot(epochs, history["val_acc"], label="Val Acc")
    plt.xlabel("Epoch")
    plt.ylabel("Accuracy")
    plt.title("Accuracy Curve")
    plt.legend()

    plt.tight_layout()
    plt.show()

plot_history(history)

"""***Transformer for Motor Imagery***"""

